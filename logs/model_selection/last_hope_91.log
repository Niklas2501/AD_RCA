
Dataset loaded:
Shape of training set (example, time, channels): (20812, 500, 61)
Shape of test set (example, time, channels): (3989, 500, 61)
Shape of train validation set (example, time, channels): (2313, 500, 61)
Shape of test validation set (example, time, channels): (1710, 500, 61)
Num of classes in all: 25

Creating model based on ../configuration/hyperparameter_combinations/last_hope_91.json hyperparameter file 

Adding feature wise convolutions with 32 filters per feature, 3 kernels and 1 strides ...
Adding feature wise convolutions with 1 filters per feature, 3 kernels and 1 strides ...
Adding feature based graph attention layer ...
Adding time based graph attention layer ...
Adding GRU layer with 256 units ...
Adding GRU layer with 61 units ...
Adding GRU AE as reconstruction model ...
Adding dense layer to forecasting model with 512 units and ReLu activation ...
Adding dense layer to forecasting model with 256 units and ReLu activation ...
Adding last dense layer to forecasting model with 61 units and sigmoid activation ...
VAE reconstructs the output of the gru layer.

Epoch 0
	Step 0: mean loss = 0.2199
	Step 50: mean loss = 0.2055
	Step 100: mean loss = 0.1471
	Step 150: mean loss = 0.1221

	Loss on validation set after epoch 0: 0.0701

Epoch 1
	Step 0: mean loss = 0.0678
	Step 50: mean loss = 0.0695
	Step 100: mean loss = 0.0692
	Step 150: mean loss = 0.0689

	Loss on validation set after epoch 1: 0.0674

Epoch 2
	Step 0: mean loss = 0.0661
	Step 50: mean loss = 0.0662
	Step 100: mean loss = 0.0650
	Step 150: mean loss = 0.0639

	Loss on validation set after epoch 2: 0.0615

Epoch 3
	Step 0: mean loss = 0.0625
	Step 50: mean loss = 0.0591
	Step 100: mean loss = 0.0579
	Step 150: mean loss = 0.0570

	Loss on validation set after epoch 3: 0.0537

Epoch 4
	Step 0: mean loss = 0.0542
	Step 50: mean loss = 0.0536
	Step 100: mean loss = 0.0536
	Step 150: mean loss = 0.0535

	Loss on validation set after epoch 4: 0.0517

Epoch 5
	Step 0: mean loss = 0.0541
	Step 50: mean loss = 0.0524
	Step 100: mean loss = 0.0522
	Step 150: mean loss = 0.0520

	Loss on validation set after epoch 5: 0.0515

Epoch 6
	Step 0: mean loss = 0.0545
	Step 50: mean loss = 0.0512
	Step 100: mean loss = 0.0510
	Step 150: mean loss = 0.0508

	Loss on validation set after epoch 6: 0.0496

Epoch 7
	Step 0: mean loss = 0.0504
	Step 50: mean loss = 0.0498
	Step 100: mean loss = 0.0496
	Step 150: mean loss = 0.0494

	Loss on validation set after epoch 7: 0.0479

Epoch 8
	Step 0: mean loss = 0.0420
	Step 50: mean loss = 0.0481
	Step 100: mean loss = 0.0479
	Step 150: mean loss = 0.0477

	Loss on validation set after epoch 8: 0.0475

Epoch 9
	Step 0: mean loss = 0.0445
	Step 50: mean loss = 0.0467
	Step 100: mean loss = 0.0469
	Step 150: mean loss = 0.0466

	Loss on validation set after epoch 9: 0.0462

Epoch 10
	Step 0: mean loss = 0.0435
	Step 50: mean loss = 0.0455
	Step 100: mean loss = 0.0451
	Step 150: mean loss = 0.0448

	Loss on validation set after epoch 10: 0.0421

Epoch 11
	Step 0: mean loss = 0.0441
	Step 50: mean loss = 0.0418
	Step 100: mean loss = 0.0416
	Step 150: mean loss = 0.0411

	Loss on validation set after epoch 11: 0.0389

Epoch 12
	Step 0: mean loss = 0.0354
	Step 50: mean loss = 0.0390
	Step 100: mean loss = 0.0385
	Step 150: mean loss = 0.0383

	Loss on validation set after epoch 12: 0.0365

Epoch 13
	Step 0: mean loss = 0.0348
	Step 50: mean loss = 0.0364
	Step 100: mean loss = 0.0362
	Step 150: mean loss = 0.0360

	Loss on validation set after epoch 13: 0.0354

Epoch 14
	Step 0: mean loss = 0.0345
	Step 50: mean loss = 0.0346
	Step 100: mean loss = 0.0346
	Step 150: mean loss = 0.0344

	Loss on validation set after epoch 14: 0.0344

Epoch 15
	Step 0: mean loss = 0.0354
	Step 50: mean loss = 0.0333
	Step 100: mean loss = 0.0336
	Step 150: mean loss = 0.0334

	Loss on validation set after epoch 15: 0.0329

Epoch 16
	Step 0: mean loss = 0.0322
	Step 50: mean loss = 0.0325
	Step 100: mean loss = 0.0324
	Step 150: mean loss = 0.0324

	Loss on validation set after epoch 16: 0.0320

Epoch 17
	Step 0: mean loss = 0.0327
	Step 50: mean loss = 0.0320
	Step 100: mean loss = 0.0319
	Step 150: mean loss = 0.0319

	Loss on validation set after epoch 17: 0.0316

Epoch 18
	Step 0: mean loss = 0.0334
	Step 50: mean loss = 0.0312
	Step 100: mean loss = 0.0312
	Step 150: mean loss = 0.0311

	Loss on validation set after epoch 18: 0.0306

Epoch 19
	Step 0: mean loss = 0.0311
	Step 50: mean loss = 0.0300
	Step 100: mean loss = 0.0304
	Step 150: mean loss = 0.0303

	Loss on validation set after epoch 19: 0.0306

Epoch 20
	Step 0: mean loss = 0.0286
	Step 50: mean loss = 0.0296
	Step 100: mean loss = 0.0298
	Step 150: mean loss = 0.0298

	Loss on validation set after epoch 20: 0.0297

Epoch 21
	Step 0: mean loss = 0.0315
	Step 50: mean loss = 0.0292
	Step 100: mean loss = 0.0291
	Step 150: mean loss = 0.0292

	Loss on validation set after epoch 21: 0.0282

Epoch 22
	Step 0: mean loss = 0.0283
	Step 50: mean loss = 0.0284
	Step 100: mean loss = 0.0283
	Step 150: mean loss = 0.0283

	Loss on validation set after epoch 22: 0.0298

Epoch 23
	Step 0: mean loss = 0.0273
	Step 50: mean loss = 0.0278
	Step 100: mean loss = 0.0277
	Step 150: mean loss = 0.0276

	Loss on validation set after epoch 23: 0.0273

Epoch 24
	Step 0: mean loss = 0.0281
	Step 50: mean loss = 0.0269
	Step 100: mean loss = 0.0269
	Step 150: mean loss = 0.0268

	Loss on validation set after epoch 24: 0.0268

Epoch 25
	Step 0: mean loss = 0.0297
	Step 50: mean loss = 0.0261
	Step 100: mean loss = 0.0262
	Step 150: mean loss = 0.0261

	Loss on validation set after epoch 25: 0.0258

Epoch 26
	Step 0: mean loss = 0.0265
	Step 50: mean loss = 0.0252
	Step 100: mean loss = 0.0254
	Step 150: mean loss = 0.0253

	Loss on validation set after epoch 26: 0.0253

Epoch 27
	Step 0: mean loss = 0.0264
	Step 50: mean loss = 0.0245
	Step 100: mean loss = 0.0245
	Step 150: mean loss = 0.0244

	Loss on validation set after epoch 27: 0.0243

Epoch 28
	Step 0: mean loss = 0.0265
	Step 50: mean loss = 0.0242
	Step 100: mean loss = 0.0240
	Step 150: mean loss = 0.0239

	Loss on validation set after epoch 28: 0.0235

Epoch 29
	Step 0: mean loss = 0.0231
	Step 50: mean loss = 0.0233
	Step 100: mean loss = 0.0232
	Step 150: mean loss = 0.0232

	Loss on validation set after epoch 29: 0.0231

Epoch 30
	Step 0: mean loss = 0.0234
	Step 50: mean loss = 0.0233
	Step 100: mean loss = 0.0230
	Step 150: mean loss = 0.0230

	Loss on validation set after epoch 30: 0.0228

Epoch 31
	Step 0: mean loss = 0.0189
	Step 50: mean loss = 0.0225
	Step 100: mean loss = 0.0224
	Step 150: mean loss = 0.0224

	Loss on validation set after epoch 31: 0.0228

Epoch 32
	Step 0: mean loss = 0.0214
	Step 50: mean loss = 0.0223
	Step 100: mean loss = 0.0223
	Step 150: mean loss = 0.0222

	Loss on validation set after epoch 32: 0.0227

Epoch 33
	Step 0: mean loss = 0.0216
	Step 50: mean loss = 0.0221
	Step 100: mean loss = 0.0220
	Step 150: mean loss = 0.0220

	Loss on validation set after epoch 33: 0.0216

Epoch 34
	Step 0: mean loss = 0.0187
	Step 50: mean loss = 0.0216
	Step 100: mean loss = 0.0215
	Step 150: mean loss = 0.0216

	Loss on validation set after epoch 34: 0.0209

Epoch 35
	Step 0: mean loss = 0.0231
	Step 50: mean loss = 0.0215
	Step 100: mean loss = 0.0213
	Step 150: mean loss = 0.0214

	Loss on validation set after epoch 35: 0.0218

Epoch 36
	Step 0: mean loss = 0.0209
	Step 50: mean loss = 0.0630
	Step 100: mean loss = 0.0775
	Step 150: mean loss = 0.0810

	Loss on validation set after epoch 36: 0.0870

Epoch 37
	Step 0: mean loss = 0.0856
	Step 50: mean loss = 0.0853
	Step 100: mean loss = 0.0843
	Step 150: mean loss = 0.0828

	Loss on validation set after epoch 37: 0.0779

Epoch 38
	Step 0: mean loss = 0.0793
	Step 50: mean loss = 0.0758
	Step 100: mean loss = 0.0748
	Step 150: mean loss = 0.0742

	Loss on validation set after epoch 38: 0.0735

Epoch 39
	Step 0: mean loss = 0.0736
	Step 50: mean loss = 0.0722
	Step 100: mean loss = 0.0712
	Step 150: mean loss = 0.0708

	Loss on validation set after epoch 39: 0.0708

Epoch 40
	Step 0: mean loss = 0.0630
	Step 50: mean loss = 0.0693
	Step 100: mean loss = 0.0692
	Step 150: mean loss = 0.0691

	Loss on validation set after epoch 40: 0.0660

Epoch 41
	Step 0: mean loss = 0.0661
	Step 50: mean loss = 0.0623
	Step 100: mean loss = 0.0614
	Step 150: mean loss = 0.0604

	Loss on validation set after epoch 41: 0.0564

Epoch 42
	Step 0: mean loss = 0.0575
	Step 50: mean loss = 0.0543
	Step 100: mean loss = 0.0520
	Step 150: mean loss = 0.0503

	Loss on validation set after epoch 42: 0.0456

Epoch 43
	Step 0: mean loss = 0.0449
	Step 50: mean loss = 0.0445
	Step 100: mean loss = 0.0439
	Step 150: mean loss = 0.0426

	Loss on validation set after epoch 43: 0.0436

Epoch 44
	Step 0: mean loss = 0.0418
	Step 50: mean loss = 0.0391
	Step 100: mean loss = 0.0386
	Step 150: mean loss = 0.0383

	Loss on validation set after epoch 44: 0.0368

Model from epoch 34 was selected by early stopping.
Training process will be stopped now.
Deleted temporary files in ../data/trained_models/temp_01-05-07-47-25/
Location of saved model: ../data/trained_models/last_hope_91_01-05-07-47-25/ 

Adding feature wise convolutions with 32 filters per feature, 3 kernels and 1 strides ...
Adding feature wise convolutions with 1 filters per feature, 3 kernels and 1 strides ...
Adding feature based graph attention layer ...
Adding time based graph attention layer ...
Adding GRU layer with 256 units ...
Adding GRU layer with 61 units ...
Adding GRU AE as reconstruction model ...
Adding dense layer to forecasting model with 512 units and ReLu activation ...
Adding dense layer to forecasting model with 256 units and ReLu activation ...
Adding last dense layer to forecasting model with 61 units and sigmoid activation ...
VAE reconstructs the output of the gru layer.

Grid search running in anomaly detection model selection mode.
The following hyperparameters will not be evaluated:
relevance_mapping
unaffected_component_threshold
si_mode
si_parameter

Testing 294 combinations via grid search 
for model last_hope_91_01-05-07-47-25/ on the validation dataset. 


Top 50 combinations tested:
      gamma  single_timestamp_a.. affected_timestamp..  AD F1  AD F2  AD TPR  AD Prec    AD ACC  AD TNR  AD FPR  SI/ST AVG-HR@100%  SI/ST AVG-HR@150%  SI/ST AVG-HR@K  SI/ST F1  SI/ST TPR  SI/ST ACC
Comb                                                                                                                                                                                                 
0       0.3                 0.200                  100    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
1       0.3                 0.200                  110    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
2       0.3                 0.200                  120    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
3       0.3                 0.200                  130    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
4       0.3                 0.200                  140    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
5       0.3                 0.200                  150    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
6       0.3                 0.200                  160    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
7       0.3                 0.225                  100    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
8       0.3                 0.225                  110    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
9       0.3                 0.225                  120    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
10      0.3                 0.225                  130    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
11      0.3                 0.225                  140    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
12      0.3                 0.225                  150    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
13      0.3                 0.225                  160    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
14      0.3                 0.250                  100    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
15      0.3                 0.250                  110    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
16      0.3                 0.250                  120    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
17      0.3                 0.250                  130    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
18      0.3                 0.250                  140    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
19      0.3                 0.250                  150    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
20      0.3                 0.250                  160    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
21      0.3                 0.275                  100    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
22      0.3                 0.275                  110    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
23      0.3                 0.275                  120    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
24      0.3                 0.275                  130    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
25      0.3                 0.275                  140    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
26      0.3                 0.275                  150    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
27      0.3                 0.275                  160    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
28      0.3                 0.300                  100    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
29      0.3                 0.300                  110    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
30      0.3                 0.300                  120    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
31      0.3                 0.300                  130    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
32      0.3                 0.300                  140    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
33      0.3                 0.300                  150    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
34      0.3                 0.300                  160    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
35      0.3                 0.325                  100    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
36      0.3                 0.325                  110    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
37      0.3                 0.325                  120    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
38      0.3                 0.325                  130    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
39      0.3                 0.325                  140    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
40      0.3                 0.325                  150    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
41      0.3                 0.325                  160    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
42      0.3                 0.350                  100    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
43      0.3                 0.350                  110    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
44      0.3                 0.350                  120    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
45      0.3                 0.350                  130    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
46      0.3                 0.350                  140    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
47      0.3                 0.350                  150    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
48      0.3                 0.350                  160    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
49      0.4                 0.200                  100    NaN    NaN     0.0      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994

Full result output for the best combination:
                 #Examples  TP  FP    TN   FN       ACC  FNR  TNR  FPR  TPR  Prec  F1  F2  AVG # affected
Component                                                                                                
no_failure            1385   0   0  1385    0  1.000000  NaN  1.0  0.0  NaN   NaN NaN NaN             0.0
txt15_i1                 5   0   0     0    5  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt15_i3                 5   0   0     0    5  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt15_conveyor           3   0   0     0    3  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt15_m1               160   0   0     0  160  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt15_pl                 9   0   0     0    9  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt16_i3                 4   0   0     0    4  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt16_conveyor           8   0   0     0    8  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt16_m3                65   0   0     0   65  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt16_turntable          2   0   0     0    2  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt17_i1                16   0   0     0   16  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt17_pl                14   0   0     0   14  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt18_pl                28   0   0     0   28  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt19_i4                 6   0   0     0    6  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
combined              1710   0   0  1385  325  0.809942  1.0  1.0  0.0  0.0   NaN NaN NaN             0.0

                 #Examples  TP  FP    TN   FN       ACC  FNR  TNR  FPR  TPR  Prec  F1  F2  AVG # affected
Component                                                                                                
no_failure            1385   0   0  1385    0  1.000000  NaN  1.0  0.0  NaN   NaN NaN NaN             0.0
txt15_i1                 5   0   0     0    5  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt15_i3                 5   0   0     0    5  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt15_conveyor           3   0   0     0    3  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt15_m1               160   0   0     0  160  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt15_pl                 9   0   0     0    9  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt16_i3                 4   0   0     0    4  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt16_conveyor           8   0   0     0    8  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt16_m3                65   0   0     0   65  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt16_turntable          2   0   0     0    2  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt17_i1                16   0   0     0   16  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt17_pl                14   0   0     0   14  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt18_pl                28   0   0     0   28  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
txt19_i4                 6   0   0     0    6  0.000000  1.0  NaN  NaN  0.0   NaN NaN NaN             0.0
combined              1710   0   0  1385  325  0.809942  1.0  1.0  0.0  0.0   NaN NaN NaN             0.0

Execution time: 3916.125148312887
