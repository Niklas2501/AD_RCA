
Dataset loaded:
Shape of training set (example, time, channels): (20812, 500, 61)
Shape of test set (example, time, channels): (3989, 500, 61)
Shape of train validation set (example, time, channels): (2313, 500, 61)
Shape of test validation set (example, time, channels): (1710, 500, 61)
Num of classes in all: 25

Creating model based on ../configuration/hyperparameter_combinations/last_hope_32.json hyperparameter file 

Adding feature wise convolutions with 32 filters per feature, 3 kernels and 1 strides ...
Adding feature wise convolutions with 1 filters per feature, 3 kernels and 1 strides ...
Adding feature based graph attention layer ...
Adding time based graph attention layer ...
Adding GRU layer with 256 units ...
Adding GRU layer with 61 units ...
Adding dense layer to forecasting model with 512 units and ReLu activation ...
Adding dense layer to forecasting model with 256 units and ReLu activation ...
Adding last dense layer to forecasting model with 61 units and sigmoid activation ...
Adding VAE as reconstruction model dimensions 1024 & 1024 ...

Epoch 0
	Step 0: mean loss = 1.2684
	Step 50: mean loss = 0.6461
	Step 100: mean loss = 0.4299
	Step 150: mean loss = 0.3304

	Loss on validation set after epoch 0: 0.1175

Epoch 1
	Step 0: mean loss = 0.1170
	Step 50: mean loss = 0.1115
	Step 100: mean loss = 0.1074
	Step 150: mean loss = 0.1037

	Loss on validation set after epoch 1: 0.0927

Epoch 2
	Step 0: mean loss = 0.0948
	Step 50: mean loss = 0.0905
	Step 100: mean loss = 0.0889
	Step 150: mean loss = 0.0875

	Loss on validation set after epoch 2: 0.0828

Epoch 3
	Step 0: mean loss = 0.0809
	Step 50: mean loss = 0.0816
	Step 100: mean loss = 0.0803
	Step 150: mean loss = 0.0792

	Loss on validation set after epoch 3: 0.0751

Epoch 4
	Step 0: mean loss = 0.0714
	Step 50: mean loss = 0.0729
	Step 100: mean loss = 0.0719
	Step 150: mean loss = 0.0711

	Loss on validation set after epoch 4: 0.0673

Epoch 5
	Step 0: mean loss = 0.0680
	Step 50: mean loss = 0.0672
	Step 100: mean loss = 0.0658
	Step 150: mean loss = 0.0647

	Loss on validation set after epoch 5: 0.0601

Epoch 6
	Step 0: mean loss = 0.0637
	Step 50: mean loss = 0.0602
	Step 100: mean loss = 0.0599
	Step 150: mean loss = 0.0596

	Loss on validation set after epoch 6: 0.0579

Epoch 7
	Step 0: mean loss = 0.0567
	Step 50: mean loss = 0.0577
	Step 100: mean loss = 0.0578
	Step 150: mean loss = 0.0576

	Loss on validation set after epoch 7: 0.0554

Epoch 8
	Step 0: mean loss = 0.0564
	Step 50: mean loss = 0.0564
	Step 100: mean loss = 0.0565
	Step 150: mean loss = 0.0563

	Loss on validation set after epoch 8: 0.0562

Epoch 9
	Step 0: mean loss = 0.0535
	Step 50: mean loss = 0.0551
	Step 100: mean loss = 0.0554
	Step 150: mean loss = 0.0551

	Loss on validation set after epoch 9: 0.0546

Epoch 10
	Step 0: mean loss = 0.0580
	Step 50: mean loss = 0.0545
	Step 100: mean loss = 0.0545
	Step 150: mean loss = 0.0545

	Loss on validation set after epoch 10: 0.0534

Epoch 11
	Step 0: mean loss = 0.0521
	Step 50: mean loss = 0.0533
	Step 100: mean loss = 0.0536
	Step 150: mean loss = 0.0536

	Loss on validation set after epoch 11: 0.0529

Epoch 12
	Step 0: mean loss = 0.0535
	Step 50: mean loss = 0.0529
	Step 100: mean loss = 0.0526
	Step 150: mean loss = 0.0524

	Loss on validation set after epoch 12: 0.0513

Epoch 13
	Step 0: mean loss = 0.0452
	Step 50: mean loss = 0.0509
	Step 100: mean loss = 0.0514
	Step 150: mean loss = 0.0512

	Loss on validation set after epoch 13: 0.0508

Epoch 14
	Step 0: mean loss = 0.0509
	Step 50: mean loss = 0.0504
	Step 100: mean loss = 0.0507
	Step 150: mean loss = 0.0508

	Loss on validation set after epoch 14: 0.0496

Epoch 15
	Step 0: mean loss = 0.0532
	Step 50: mean loss = 0.0495
	Step 100: mean loss = 0.0495
	Step 150: mean loss = 0.0494

	Loss on validation set after epoch 15: 0.0486

Epoch 16
	Step 0: mean loss = 0.0475
	Step 50: mean loss = 0.0486
	Step 100: mean loss = 0.0487
	Step 150: mean loss = 0.0486

	Loss on validation set after epoch 16: 0.0488

Epoch 17
	Step 0: mean loss = 0.0480
	Step 50: mean loss = 0.0473
	Step 100: mean loss = 0.0470
	Step 150: mean loss = 0.0468

	Loss on validation set after epoch 17: 0.0460

Epoch 18
	Step 0: mean loss = 0.0432
	Step 50: mean loss = 0.0452
	Step 100: mean loss = 0.0450
	Step 150: mean loss = 0.0446

	Loss on validation set after epoch 18: 0.0445

Epoch 19
	Step 0: mean loss = 0.0458
	Step 50: mean loss = 0.0431
	Step 100: mean loss = 0.0427
	Step 150: mean loss = 0.0424

	Loss on validation set after epoch 19: 0.0415

Epoch 20
	Step 0: mean loss = 0.0399
	Step 50: mean loss = 0.0402
	Step 100: mean loss = 0.0401
	Step 150: mean loss = 0.0399

	Loss on validation set after epoch 20: 0.0391

Epoch 21
	Step 0: mean loss = 0.0397
	Step 50: mean loss = 0.0382
	Step 100: mean loss = 0.0383
	Step 150: mean loss = 0.0381

	Loss on validation set after epoch 21: 0.0373

Epoch 22
	Step 0: mean loss = 0.0370
	Step 50: mean loss = 0.0367
	Step 100: mean loss = 0.0366
	Step 150: mean loss = 0.0367

	Loss on validation set after epoch 22: 0.0362

Epoch 23
	Step 0: mean loss = 0.0378
	Step 50: mean loss = 0.0359
	Step 100: mean loss = 0.0359
	Step 150: mean loss = 0.0358

	Loss on validation set after epoch 23: 0.0350

Epoch 24
	Step 0: mean loss = 0.0336
	Step 50: mean loss = 0.0350
	Step 100: mean loss = 0.0347
	Step 150: mean loss = 0.0346

	Loss on validation set after epoch 24: 0.0340

Epoch 25
	Step 0: mean loss = 0.0352
	Step 50: mean loss = 0.0335
	Step 100: mean loss = 0.0337
	Step 150: mean loss = 0.0336

	Loss on validation set after epoch 25: 0.0331

Epoch 26
	Step 0: mean loss = 0.0278
	Step 50: mean loss = 0.0326
	Step 100: mean loss = 0.0327
	Step 150: mean loss = 0.0328

	Loss on validation set after epoch 26: 0.0323

Epoch 27
	Step 0: mean loss = 0.0317
	Step 50: mean loss = 0.0320
	Step 100: mean loss = 0.0320
	Step 150: mean loss = 0.0318

	Loss on validation set after epoch 27: 0.0321

Epoch 28
	Step 0: mean loss = 0.0361
	Step 50: mean loss = 0.0311
	Step 100: mean loss = 0.0312
	Step 150: mean loss = 0.0311

	Loss on validation set after epoch 28: 0.0315

Epoch 29
	Step 0: mean loss = 0.0334
	Step 50: mean loss = 0.0302
	Step 100: mean loss = 0.0305
	Step 150: mean loss = 0.0304

	Loss on validation set after epoch 29: 0.0303

Epoch 30
	Step 0: mean loss = 0.0248
	Step 50: mean loss = 0.0300
	Step 100: mean loss = 0.0298
	Step 150: mean loss = 0.0298

	Loss on validation set after epoch 30: 0.0305

Epoch 31
	Step 0: mean loss = 0.0270
	Step 50: mean loss = 0.0297
	Step 100: mean loss = 0.0294
	Step 150: mean loss = 0.0294

	Loss on validation set after epoch 31: 0.0290

Epoch 32
	Step 0: mean loss = 0.0301
	Step 50: mean loss = 0.0286
	Step 100: mean loss = 0.0285
	Step 150: mean loss = 0.0285

	Loss on validation set after epoch 32: 0.0279

Epoch 33
	Step 0: mean loss = 0.0265
	Step 50: mean loss = 0.0276
	Step 100: mean loss = 0.0275
	Step 150: mean loss = 0.0276

	Loss on validation set after epoch 33: 0.0271

Epoch 34
	Step 0: mean loss = 0.0281
	Step 50: mean loss = 0.0271
	Step 100: mean loss = 0.0271
	Step 150: mean loss = 0.0270

	Loss on validation set after epoch 34: 0.0274

Epoch 35
	Step 0: mean loss = 0.0299
	Step 50: mean loss = 0.0267
	Step 100: mean loss = 0.0268
	Step 150: mean loss = 0.0267

	Loss on validation set after epoch 35: 0.0268

Epoch 36
	Step 0: mean loss = 0.0284
	Step 50: mean loss = 0.0261
	Step 100: mean loss = 0.0264
	Step 150: mean loss = 0.0263

	Loss on validation set after epoch 36: 0.0256

Epoch 37
	Step 0: mean loss = 0.0262
	Step 50: mean loss = 0.0257
	Step 100: mean loss = 0.0257
	Step 150: mean loss = 0.0256

	Loss on validation set after epoch 37: 0.0256

Epoch 38
	Step 0: mean loss = 0.0243
	Step 50: mean loss = 0.0251
	Step 100: mean loss = 0.0253
	Step 150: mean loss = 0.0253

	Loss on validation set after epoch 38: 0.0248

Epoch 39
	Step 0: mean loss = 0.0246
	Step 50: mean loss = 0.0248
	Step 100: mean loss = 0.0249
	Step 150: mean loss = 0.0249

	Loss on validation set after epoch 39: 0.0251

Epoch 40
	Step 0: mean loss = 0.0278
	Step 50: mean loss = 0.0245
	Step 100: mean loss = 0.0246
	Step 150: mean loss = 0.0246

	Loss on validation set after epoch 40: 0.0247

Epoch 41
	Step 0: mean loss = 0.0253
	Step 50: mean loss = 0.0241
	Step 100: mean loss = 0.0242
	Step 150: mean loss = 0.0243

	Loss on validation set after epoch 41: 0.0240

Epoch 42
	Step 0: mean loss = 0.0228
	Step 50: mean loss = 0.0243
	Step 100: mean loss = 0.0242
	Step 150: mean loss = 0.0241

	Loss on validation set after epoch 42: 0.0244

Epoch 43
	Step 0: mean loss = 0.0247
	Step 50: mean loss = 0.0237
	Step 100: mean loss = 0.0237
	Step 150: mean loss = 0.0238

	Loss on validation set after epoch 43: 0.0232

Epoch 44
	Step 0: mean loss = 0.0208
	Step 50: mean loss = 0.0232
	Step 100: mean loss = 0.0235
	Step 150: mean loss = 0.0235

	Loss on validation set after epoch 44: 0.0228

Epoch 45
	Step 0: mean loss = 0.0186
	Step 50: mean loss = 0.0233
	Step 100: mean loss = 0.0233
	Step 150: mean loss = 0.0233

	Loss on validation set after epoch 45: 0.0232

Epoch 46
	Step 0: mean loss = 0.0216
	Step 50: mean loss = 0.0232
	Step 100: mean loss = 0.0232
	Step 150: mean loss = 0.0232

	Loss on validation set after epoch 46: 0.0235

Epoch 47
	Step 0: mean loss = 0.0210
	Step 50: mean loss = 0.0227
	Step 100: mean loss = 0.0228
	Step 150: mean loss = 0.0228

	Loss on validation set after epoch 47: 0.0244

Epoch 48
	Step 0: mean loss = 0.0255
	Step 50: mean loss = 0.0229
	Step 100: mean loss = 0.0231
	Step 150: mean loss = 0.0235

	Loss on validation set after epoch 48: 0.0226

Epoch 49
	Step 0: mean loss = 0.0242
	Step 50: mean loss = 0.0228
	Step 100: mean loss = 0.0228
	Step 150: mean loss = 0.0228

	Loss on validation set after epoch 49: 0.0223

Epoch 50
	Step 0: mean loss = 0.0205
	Step 50: mean loss = 0.0225
	Step 100: mean loss = 0.0227
	Step 150: mean loss = 0.0226

	Loss on validation set after epoch 50: 0.0225

Epoch 51
	Step 0: mean loss = 0.0256
	Step 50: mean loss = 0.0224
	Step 100: mean loss = 0.0225
	Step 150: mean loss = 0.0224

	Loss on validation set after epoch 51: 0.0228

Epoch 52
	Step 0: mean loss = 0.0252
	Step 50: mean loss = 0.0221
	Step 100: mean loss = 0.0222
	Step 150: mean loss = 0.0223

	Loss on validation set after epoch 52: 0.0221

Epoch 53
	Step 0: mean loss = 0.0237
	Step 50: mean loss = 0.0220
	Step 100: mean loss = 0.0223
	Step 150: mean loss = 0.0222

	Loss on validation set after epoch 53: 0.0235

Epoch 54
	Step 0: mean loss = 0.0238
	Step 50: mean loss = 0.0221
	Step 100: mean loss = 0.0222
	Step 150: mean loss = 0.0221

	Loss on validation set after epoch 54: 0.0230

Epoch 55
	Step 0: mean loss = 0.0202
	Step 50: mean loss = 0.0217
	Step 100: mean loss = 0.0218
	Step 150: mean loss = 0.0218

	Loss on validation set after epoch 55: 0.0224

Epoch 56
	Step 0: mean loss = 0.0212
	Step 50: mean loss = 0.0218
	Step 100: mean loss = 0.0218
	Step 150: mean loss = 0.0219

	Loss on validation set after epoch 56: 0.0223

Epoch 57
	Step 0: mean loss = 0.0198
	Step 50: mean loss = 0.0217
	Step 100: mean loss = 0.0216
	Step 150: mean loss = 0.0217

	Loss on validation set after epoch 57: 0.0218

Epoch 58
	Step 0: mean loss = 0.0228
	Step 50: mean loss = 0.0219
	Step 100: mean loss = 0.0217
	Step 150: mean loss = 0.0217

	Loss on validation set after epoch 58: 0.0221

Epoch 59
	Step 0: mean loss = 0.0214
	Step 50: mean loss = 0.0214
	Step 100: mean loss = 0.0214
	Step 150: mean loss = 0.0215

	Loss on validation set after epoch 59: 0.0216

Epoch 60
	Step 0: mean loss = 0.0194
	Step 50: mean loss = 0.0213
	Step 100: mean loss = 0.0214
	Step 150: mean loss = 0.0214

	Loss on validation set after epoch 60: 0.0216

Epoch 61
	Step 0: mean loss = 0.0233
	Step 50: mean loss = 0.0211
	Step 100: mean loss = 0.0212
	Step 150: mean loss = 0.0212

	Loss on validation set after epoch 61: 0.0217

Epoch 62
	Step 0: mean loss = 0.0218
	Step 50: mean loss = 0.0211
	Step 100: mean loss = 0.0212
	Step 150: mean loss = 0.0212

	Loss on validation set after epoch 62: 0.0214

Epoch 63
	Step 0: mean loss = 0.0215
	Step 50: mean loss = 0.0212
	Step 100: mean loss = 0.0212
	Step 150: mean loss = 0.0212

	Loss on validation set after epoch 63: 0.0215

Epoch 64
	Step 0: mean loss = 0.0216
	Step 50: mean loss = 0.0210
	Step 100: mean loss = 0.0215
	Step 150: mean loss = 0.0214

	Loss on validation set after epoch 64: 0.0210

Epoch 65
	Step 0: mean loss = 0.0189
	Step 50: mean loss = 0.0209
	Step 100: mean loss = 0.0209
	Step 150: mean loss = 0.0208

	Loss on validation set after epoch 65: 0.0209

Epoch 66
	Step 0: mean loss = 0.0211
	Step 50: mean loss = 0.0210
	Step 100: mean loss = 0.0211
	Step 150: mean loss = 0.0210

	Loss on validation set after epoch 66: 0.0210

Epoch 67
	Step 0: mean loss = 0.0222
	Step 50: mean loss = 0.0206
	Step 100: mean loss = 0.0206
	Step 150: mean loss = 0.0207

	Loss on validation set after epoch 67: 0.0208

Epoch 68
	Step 0: mean loss = 0.0187
	Step 50: mean loss = 0.0207
	Step 100: mean loss = 0.0208
	Step 150: mean loss = 0.0209

	Loss on validation set after epoch 68: 0.0212

Epoch 69
	Step 0: mean loss = 0.0209
	Step 50: mean loss = 0.0206
	Step 100: mean loss = 0.0210
	Step 150: mean loss = 0.0207

	Loss on validation set after epoch 69: 0.0210

Epoch 70
	Step 0: mean loss = 0.0177
	Step 50: mean loss = 0.0203
	Step 100: mean loss = 0.0204
	Step 150: mean loss = 0.0205

	Loss on validation set after epoch 70: 0.0205

Epoch 71
	Step 0: mean loss = 0.0211
	Step 50: mean loss = 0.0206
	Step 100: mean loss = 0.0205
	Step 150: mean loss = 0.0205

	Loss on validation set after epoch 71: 0.0203

Epoch 72
	Step 0: mean loss = 0.0228
	Step 50: mean loss = 0.0200
	Step 100: mean loss = 0.0203
	Step 150: mean loss = 0.0202

	Loss on validation set after epoch 72: 0.0207

Epoch 73
	Step 0: mean loss = 0.0208
	Step 50: mean loss = 0.0204
	Step 100: mean loss = 0.0204
	Step 150: mean loss = 0.0203

	Loss on validation set after epoch 73: 0.0215

Epoch 74
	Step 0: mean loss = 0.0221
	Step 50: mean loss = 0.0200
	Step 100: mean loss = 0.0201
	Step 150: mean loss = 0.0200

	Loss on validation set after epoch 74: 0.0204

Epoch 75
	Step 0: mean loss = 0.0234
	Step 50: mean loss = 0.0203
	Step 100: mean loss = 0.0202
	Step 150: mean loss = 0.0202

	Loss on validation set after epoch 75: 0.0205

Epoch 76
	Step 0: mean loss = 0.0188
	Step 50: mean loss = 0.0201
	Step 100: mean loss = 0.0202
	Step 150: mean loss = 0.0204

	Loss on validation set after epoch 76: 0.0201

Epoch 77
	Step 0: mean loss = 0.0165
	Step 50: mean loss = 0.0199
	Step 100: mean loss = 0.0201
	Step 150: mean loss = 0.0199

	Loss on validation set after epoch 77: 0.0196

Epoch 78
	Step 0: mean loss = 0.0191
	Step 50: mean loss = 0.0197
	Step 100: mean loss = 0.0197
	Step 150: mean loss = 0.0198

	Loss on validation set after epoch 78: 0.0205

Epoch 79
	Step 0: mean loss = 0.0210
	Step 50: mean loss = 0.0197
	Step 100: mean loss = 0.0199
	Step 150: mean loss = 0.0198

	Loss on validation set after epoch 79: 0.0203

Epoch 80
	Step 0: mean loss = 0.0192
	Step 50: mean loss = 0.0196
	Step 100: mean loss = 0.0197
	Step 150: mean loss = 0.0199

	Loss on validation set after epoch 80: 0.0206

Epoch 81
	Step 0: mean loss = 0.0204
	Step 50: mean loss = 0.0197
	Step 100: mean loss = 0.0201
	Step 150: mean loss = 0.0200

	Loss on validation set after epoch 81: 0.0198

Epoch 82
	Step 0: mean loss = 0.0183
	Step 50: mean loss = 0.0197
	Step 100: mean loss = 0.0196
	Step 150: mean loss = 0.0198

	Loss on validation set after epoch 82: 0.0202

Epoch 83
	Step 0: mean loss = 0.0192
	Step 50: mean loss = 0.0193
	Step 100: mean loss = 0.0193
	Step 150: mean loss = 0.0193

	Loss on validation set after epoch 83: 0.0205

Epoch 84
	Step 0: mean loss = 0.0223
	Step 50: mean loss = 0.0195
	Step 100: mean loss = 0.0194
	Step 150: mean loss = 0.0193

	Loss on validation set after epoch 84: 0.0198

Epoch 85
	Step 0: mean loss = 0.0215
	Step 50: mean loss = 0.0191
	Step 100: mean loss = 0.0192
	Step 150: mean loss = 0.0192

	Loss on validation set after epoch 85: 0.0203

Epoch 86
	Step 0: mean loss = 0.0206
	Step 50: mean loss = 0.0195
	Step 100: mean loss = 0.0194
	Step 150: mean loss = 0.0193

	Loss on validation set after epoch 86: 0.0199

Epoch 87
	Step 0: mean loss = 0.0207
	Step 50: mean loss = 0.0192
	Step 100: mean loss = 0.0192
	Step 150: mean loss = 0.0191

	Loss on validation set after epoch 87: 0.0204

Model from epoch 77 was selected by early stopping.
Training process will be stopped now.
Deleted temporary files in ../data/trained_models/temp_01-01-19-26-16/
Location of saved model: ../data/trained_models/last_hope_32_01-01-19-26-16/ 

Adding feature wise convolutions with 32 filters per feature, 3 kernels and 1 strides ...
Adding feature wise convolutions with 1 filters per feature, 3 kernels and 1 strides ...
Adding feature based graph attention layer ...
Adding time based graph attention layer ...
Adding GRU layer with 256 units ...
Adding GRU layer with 61 units ...
Adding dense layer to forecasting model with 512 units and ReLu activation ...
Adding dense layer to forecasting model with 256 units and ReLu activation ...
Adding last dense layer to forecasting model with 61 units and sigmoid activation ...
Adding VAE as reconstruction model dimensions 1024 & 1024 ...

Grid search running in anomaly detection model selection mode.
The following hyperparameters will not be evaluated:
relevance_mapping
unaffected_component_threshold
si_mode
si_parameter

Testing 294 combinations via grid search 
for model last_hope_32_01-01-19-26-16/ on the validation dataset. 


Top 50 combinations tested:
      gamma  single_timestamp_a.. affected_timestamp..     AD F1     AD F2    AD TPR   AD Prec    AD ACC    AD TNR    AD FPR  SI/ST AVG-HR@100%  SI/ST AVG-HR@150%  SI/ST AVG-HR@K  SI/ST F1  SI/ST TPR  SI/ST ACC
Comb                                                                                                                                                                                                              
16      0.3                 0.250                  120  0.481613  0.558306  0.624615  0.391892  0.744444  0.772563  0.227437           0.355408           0.446167        0.467692  0.206154   0.206154   0.877870
15      0.3                 0.250                  110  0.480747  0.562227  0.633846  0.387218  0.739766  0.764621  0.235379           0.355408           0.446167        0.467692  0.206154   0.206154   0.877870
14      0.3                 0.250                  100  0.480549  0.567875  0.646154  0.382514  0.734503  0.755235  0.244765           0.355408           0.446167        0.467692  0.206154   0.206154   0.877870
18      0.3                 0.250                  140  0.475956  0.540314  0.593846  0.397119  0.751462  0.788448  0.211552           0.355408           0.446167        0.467692  0.206154   0.206154   0.877870
17      0.3                 0.250                  130  0.475152  0.544444  0.603077  0.392000  0.746784  0.780505  0.219495           0.355408           0.446167        0.467692  0.206154   0.206154   0.877870
19      0.3                 0.250                  150  0.469413  0.529279  0.578462  0.394958  0.751462  0.792058  0.207942           0.355408           0.446167        0.467692  0.206154   0.206154   0.877870
20      0.3                 0.250                  160  0.461146  0.514205  0.556923  0.393478  0.752632  0.798556  0.201444           0.355408           0.446167        0.467692  0.206154   0.206154   0.877870
122     0.5                 0.275                  130  0.405941  0.562414  0.756923  0.277339  0.578947  0.537184  0.462816           0.359992           0.430265        0.455385  0.169231   0.169231   0.872189
68      0.4                 0.250                  150  0.405461  0.608607  0.913846  0.260526  0.490643  0.391336  0.608664           0.356642           0.437121        0.473846  0.190769   0.190769   0.875503
69      0.4                 0.250                  160  0.405220  0.606746  0.907692  0.260831  0.493567  0.396390  0.603610           0.356642           0.437121        0.473846  0.190769   0.190769   0.875503
67      0.4                 0.250                  140  0.405167  0.609158  0.916923  0.260035  0.488304  0.387726  0.612274           0.356642           0.437121        0.473846  0.190769   0.190769   0.875503
123     0.5                 0.275                  140  0.405063  0.555556  0.738462  0.279070  0.587719  0.552347  0.447653           0.359992           0.430265        0.455385  0.169231   0.169231   0.872189
121     0.5                 0.275                  120  0.403595  0.561619  0.760000  0.274750  0.573099  0.529242  0.470758           0.359992           0.430265        0.455385  0.169231   0.169231   0.872189
66      0.4                 0.250                  130  0.402975  0.607172  0.916923  0.258232  0.483626  0.381949  0.618051           0.356642           0.437121        0.473846  0.190769   0.190769   0.875503
64      0.4                 0.250                  110  0.402138  0.608819  0.926154  0.256826  0.476608  0.371119  0.628881           0.356642           0.437121        0.473846  0.190769   0.190769   0.875503
65      0.4                 0.250                  120  0.401617  0.605937  0.916923  0.257118  0.480702  0.378339  0.621661           0.356642           0.437121        0.473846  0.190769   0.190769   0.875503
119     0.5                 0.275                  100  0.401587  0.565996  0.778462  0.270588  0.559064  0.507581  0.492419           0.359992           0.430265        0.455385  0.169231   0.169231   0.872189
120     0.5                 0.275                  110  0.400644  0.561317  0.766154  0.271242  0.564327  0.516968  0.483032           0.359992           0.430265        0.455385  0.169231   0.169231   0.872189
63      0.4                 0.250                  100  0.399471  0.607157  0.929231  0.254423  0.469006  0.361011  0.638989           0.356642           0.437121        0.473846  0.190769   0.190769   0.875503
124     0.5                 0.275                  150  0.397590  0.540477  0.710769  0.275986  0.590643  0.562455  0.437545           0.359992           0.430265        0.455385  0.169231   0.169231   0.872189
125     0.5                 0.275                  160  0.396853  0.535630  0.698462  0.277167  0.596491  0.572563  0.427437           0.359992           0.430265        0.455385  0.169231   0.169231   0.872189
13      0.3                 0.225                  160  0.393152  0.607367  0.953846  0.247604  0.440351  0.319856  0.680144           0.351314           0.445900        0.489231  0.212308   0.212308   0.878817
12      0.3                 0.225                  150  0.391167  0.605469  0.953846  0.246032  0.435673  0.314079  0.685921           0.351314           0.445900        0.489231  0.212308   0.212308   0.878817
11      0.3                 0.225                  140  0.390244  0.606061  0.960000  0.244898  0.429825  0.305415  0.694585           0.351314           0.445900        0.489231  0.212308   0.212308   0.878817
10      0.3                 0.225                  130  0.387337  0.603248  0.960000  0.242613  0.422807  0.296751  0.703249           0.351314           0.445900        0.489231  0.212308   0.212308   0.878817
9       0.3                 0.225                  120  0.383057  0.599078  0.960000  0.239264  0.412281  0.283755  0.716245           0.351314           0.445900        0.489231  0.212308   0.212308   0.878817
8       0.3                 0.225                  110  0.381475  0.598242  0.963077  0.237842  0.406433  0.275812  0.724188           0.351314           0.445900        0.489231  0.212308   0.212308   0.878817
7       0.3                 0.225                  100  0.377631  0.595148  0.966154  0.234679  0.394737  0.260650  0.739350           0.351314           0.445900        0.489231  0.212308   0.212308   0.878817
172     0.6                 0.275                  140  0.372700  0.590226  0.966154  0.230882  0.381871  0.244765  0.755235           0.354843           0.425626        0.436923  0.163077   0.163077   0.871243
174     0.6                 0.275                  160  0.372372  0.587121  0.953846  0.231343  0.388889  0.256318  0.743682           0.354843           0.425626        0.436923  0.163077   0.163077   0.871243
173     0.6                 0.275                  150  0.371122  0.586571  0.956923  0.230200  0.383626  0.249097  0.750903           0.354843           0.425626        0.436923  0.163077   0.163077   0.871243
171     0.6                 0.275                  130  0.370501  0.588015  0.966154  0.229197  0.376023  0.237545  0.762455           0.354843           0.425626        0.436923  0.163077   0.163077   0.871243
170     0.6                 0.275                  120  0.369285  0.587467  0.969231  0.228096  0.370760  0.230325  0.769675           0.354843           0.425626        0.436923  0.163077   0.163077   0.871243
169     0.6                 0.275                  110  0.366802  0.585619  0.972308  0.226037  0.361988  0.218773  0.781227           0.354843           0.425626        0.436923  0.163077   0.163077   0.871243
168     0.6                 0.275                  100  0.363427  0.582167  0.972308  0.223479  0.352632  0.207220  0.792780           0.354843           0.425626        0.436923  0.163077   0.163077   0.871243
275     0.8                 0.300                  120  0.353993  0.559744  0.913846  0.219512  0.366082  0.237545  0.762455           0.347520           0.422550        0.406154  0.147692   0.147692   0.868876
276     0.8                 0.300                  130  0.353438  0.556400  0.901538  0.219805  0.373099  0.249097  0.750903           0.347520           0.422550        0.406154  0.147692   0.147692   0.868876
274     0.8                 0.300                  110  0.353011  0.560135  0.920000  0.218408  0.359064  0.227437  0.772563           0.347520           0.422550        0.406154  0.147692   0.147692   0.868876
277     0.8                 0.300                  140  0.352439  0.552581  0.889231  0.219772  0.378947  0.259206  0.740794           0.347520           0.422550        0.406154  0.147692   0.147692   0.868876
278     0.8                 0.300                  150  0.352363  0.551075  0.883077  0.220092  0.383041  0.265704  0.734296           0.347520           0.422550        0.406154  0.147692   0.147692   0.868876
273     0.8                 0.300                  100  0.350204  0.558649  0.926154  0.215925  0.346784  0.210830  0.789170           0.347520           0.422550        0.406154  0.147692   0.147692   0.868876
279     0.8                 0.300                  160  0.349127  0.542846  0.861538  0.218921  0.389474  0.278700  0.721300           0.347520           0.422550        0.406154  0.147692   0.147692   0.868876
118     0.5                 0.250                  160  0.335942  0.556708  0.990769  0.202261  0.255556  0.083032  0.916968           0.351224           0.429355        0.461538  0.172308   0.172308   0.872663
117     0.5                 0.250                  150  0.335591  0.556323  0.990769  0.202008  0.254386  0.081588  0.918412           0.351224           0.429355        0.461538  0.172308   0.172308   0.872663
116     0.5                 0.250                  140  0.334893  0.555556  0.990769  0.201502  0.252047  0.078700  0.921300           0.351224           0.429355        0.461538  0.172308   0.172308   0.872663
115     0.5                 0.250                  130  0.334372  0.554981  0.990769  0.201124  0.250292  0.076534  0.923466           0.351224           0.429355        0.461538  0.172308   0.172308   0.872663
223     0.7                 0.275                  160  0.334196  0.555365  0.993846  0.200871  0.247368  0.072202  0.927798           0.352277           0.423558        0.430769  0.147692   0.147692   0.868876
224     0.7                 0.300                  100  0.334056  0.489822  0.710769  0.218336  0.461404  0.402888  0.597112           0.361824           0.429834        0.415385  0.138462   0.138462   0.867456
175     0.6                 0.300                  100  0.333720  0.391730  0.443077  0.267658  0.663743  0.715523  0.284477           0.366168           0.435010        0.403077  0.156923   0.156923   0.870296
114     0.5                 0.250                  120  0.333679  0.554217  0.990769  0.200623  0.247953  0.073646  0.926354           0.351224           0.429355        0.461538  0.172308   0.172308   0.872663

Full result output for the best combination:
                 #Examples   TP   FP    TN   FN       ACC       FNR       TNR       FPR       TPR      Prec        F1        F2  AVG # affected
Component                                                                                                                                      
no_failure            1385    0  315  1070    0  0.772563       NaN  0.772563  0.227437       NaN  0.000000       NaN       NaN       85.702527
txt15_i1                 5    5    0     0    0  1.000000  0.000000       NaN       NaN  1.000000  1.000000  1.000000  1.000000      304.800000
txt15_i3                 5    1    0     0    4  0.200000  0.800000       NaN       NaN  0.200000  1.000000  0.333333  0.238095       31.400000
txt15_conveyor           3    3    0     0    0  1.000000  0.000000       NaN       NaN  1.000000  1.000000  1.000000  1.000000      352.666667
txt15_m1               160  126    0     0   34  0.787500  0.212500       NaN       NaN  0.787500  1.000000  0.881119  0.822454      314.575000
txt15_pl                 9    4    0     0    5  0.444444  0.555556       NaN       NaN  0.444444  1.000000  0.615385  0.500000      177.111111
txt16_i3                 4    0    0     0    4  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        5.750000
txt16_conveyor           8    2    0     0    6  0.250000  0.750000       NaN       NaN  0.250000  1.000000  0.400000  0.294118       70.750000
txt16_m3                65   33    0     0   32  0.507692  0.492308       NaN       NaN  0.507692  1.000000  0.673469  0.563140      190.923077
txt16_turntable          2    0    0     0    2  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        7.500000
txt17_i1                16    2    0     0   14  0.125000  0.875000       NaN       NaN  0.125000  1.000000  0.222222  0.151515       56.187500
txt17_pl                14    1    0     0   13  0.071429  0.928571       NaN       NaN  0.071429  1.000000  0.133333  0.087719       26.071429
txt18_pl                28   26    0     0    2  0.928571  0.071429       NaN       NaN  0.928571  1.000000  0.962963  0.942029      336.214286
txt19_i4                 6    0    0     0    6  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        4.500000
combined              1710  203  315  1070  122  0.744444  0.375385  0.772563  0.227437  0.624615  0.391892  0.481613  0.558306      115.252632

                 #Examples   TP   FP    TN   FN       ACC       FNR       TNR       FPR       TPR      Prec        F1        F2  AVG # affected
Component                                                                                                                                      
no_failure            1385    0  315  1070    0  0.772563       NaN  0.772563  0.227437       NaN  0.000000       NaN       NaN       85.702527
txt15_i1                 5    5    0     0    0  1.000000  0.000000       NaN       NaN  1.000000  1.000000  1.000000  1.000000      304.800000
txt15_i3                 5    1    0     0    4  0.200000  0.800000       NaN       NaN  0.200000  1.000000  0.333333  0.238095       31.400000
txt15_conveyor           3    3    0     0    0  1.000000  0.000000       NaN       NaN  1.000000  1.000000  1.000000  1.000000      352.666667
txt15_m1               160  126    0     0   34  0.787500  0.212500       NaN       NaN  0.787500  1.000000  0.881119  0.822454      314.575000
txt15_pl                 9    4    0     0    5  0.444444  0.555556       NaN       NaN  0.444444  1.000000  0.615385  0.500000      177.111111
txt16_i3                 4    0    0     0    4  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        5.750000
txt16_conveyor           8    2    0     0    6  0.250000  0.750000       NaN       NaN  0.250000  1.000000  0.400000  0.294118       70.750000
txt16_m3                65   33    0     0   32  0.507692  0.492308       NaN       NaN  0.507692  1.000000  0.673469  0.563140      190.923077
txt16_turntable          2    0    0     0    2  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        7.500000
txt17_i1                16    2    0     0   14  0.125000  0.875000       NaN       NaN  0.125000  1.000000  0.222222  0.151515       56.187500
txt17_pl                14    1    0     0   13  0.071429  0.928571       NaN       NaN  0.071429  1.000000  0.133333  0.087719       26.071429
txt18_pl                28   26    0     0    2  0.928571  0.071429       NaN       NaN  0.928571  1.000000  0.962963  0.942029      336.214286
txt19_i4                 6    0    0     0    6  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        4.500000
combined              1710  203  315  1070  122  0.744444  0.375385  0.772563  0.227437  0.624615  0.391892  0.481613  0.558306      115.252632

Execution time: 6980.161687429063
