
Dataset loaded:
Shape of training set (example, time, channels): (20812, 500, 61)
Shape of test set (example, time, channels): (3989, 500, 61)
Shape of train validation set (example, time, channels): (2313, 500, 61)
Shape of test validation set (example, time, channels): (1710, 500, 61)
Num of classes in all: 25

Creating model based on ../configuration/hyperparameter_combinations/last_hope_71.json hyperparameter file 

Adding feature wise convolutions with 32 filters per feature, 3 kernels and 1 strides ...
Adding feature wise convolutions with 1 filters per feature, 3 kernels and 1 strides ...
Adding feature based graph attention layer ...
Adding time based graph attention layer ...
Adding GRU layer with 256 units ...
Adding GRU layer with 61 units ...
Adding VAE as reconstruction model dimensions 3072 & 1536 and ...
Adding GRU layer to forecasting model with 512 units ...
Adding GRU layer to forecasting model with 256 units ...
Adding GRU layer to forecasting model with 61 units ...
VAE reconstructs the output of the gru layer.

Epoch 0
	Step 0: mean loss = 1.3108
	Step 50: mean loss = 0.4865
	Step 100: mean loss = 0.3362
	Step 150: mean loss = 0.2736

	Loss on validation set after epoch 0: 0.1310

Epoch 1
	Step 0: mean loss = 0.1313
	Step 50: mean loss = 0.1230
	Step 100: mean loss = 0.1172
	Step 150: mean loss = 0.1124

	Loss on validation set after epoch 1: 0.0979

Epoch 2
	Step 0: mean loss = 0.0990
	Step 50: mean loss = 0.0954
	Step 100: mean loss = 0.0935
	Step 150: mean loss = 0.0917

	Loss on validation set after epoch 2: 0.0863

Epoch 3
	Step 0: mean loss = 0.0850
	Step 50: mean loss = 0.0851
	Step 100: mean loss = 0.0842
	Step 150: mean loss = 0.0836

	Loss on validation set after epoch 3: 0.0813

Epoch 4
	Step 0: mean loss = 0.0793
	Step 50: mean loss = 0.0804
	Step 100: mean loss = 0.0800
	Step 150: mean loss = 0.0796

	Loss on validation set after epoch 4: 0.0776

Epoch 5
	Step 0: mean loss = 0.0764
	Step 50: mean loss = 0.0779
	Step 100: mean loss = 0.0776
	Step 150: mean loss = 0.0774

	Loss on validation set after epoch 5: 0.0759

Epoch 6
	Step 0: mean loss = 0.0786
	Step 50: mean loss = 0.0764
	Step 100: mean loss = 0.0762
	Step 150: mean loss = 0.0761

	Loss on validation set after epoch 6: 0.0756

Epoch 7
	Step 0: mean loss = 0.0742
	Step 50: mean loss = 0.0753
	Step 100: mean loss = 0.0753
	Step 150: mean loss = 0.0752

	Loss on validation set after epoch 7: 0.0742

Epoch 8
	Step 0: mean loss = 0.0739
	Step 50: mean loss = 0.0746
	Step 100: mean loss = 0.0747
	Step 150: mean loss = 0.0746

	Loss on validation set after epoch 8: 0.0752

Epoch 9
	Step 0: mean loss = 0.0719
	Step 50: mean loss = 0.0740
	Step 100: mean loss = 0.0742
	Step 150: mean loss = 0.0740

	Loss on validation set after epoch 9: 0.0748

Epoch 10
	Step 0: mean loss = 0.0775
	Step 50: mean loss = 0.0738
	Step 100: mean loss = 0.0738
	Step 150: mean loss = 0.0738

	Loss on validation set after epoch 10: 0.0742

Epoch 11
	Step 0: mean loss = 0.0728
	Step 50: mean loss = 0.0732
	Step 100: mean loss = 0.0734
	Step 150: mean loss = 0.0734

	Loss on validation set after epoch 11: 0.0731

Epoch 12
	Step 0: mean loss = 0.0743
	Step 50: mean loss = 0.0732
	Step 100: mean loss = 0.0732
	Step 150: mean loss = 0.0732

	Loss on validation set after epoch 12: 0.0734

Epoch 13
	Step 0: mean loss = 0.0670
	Step 50: mean loss = 0.0729
	Step 100: mean loss = 0.0730
	Step 150: mean loss = 0.0731

	Loss on validation set after epoch 13: 0.0732

Epoch 14
	Step 0: mean loss = 0.0734
	Step 50: mean loss = 0.0729
	Step 100: mean loss = 0.0728
	Step 150: mean loss = 0.0729

	Loss on validation set after epoch 14: 0.0721

Epoch 15
	Step 0: mean loss = 0.0755
	Step 50: mean loss = 0.0724
	Step 100: mean loss = 0.0726
	Step 150: mean loss = 0.0726

	Loss on validation set after epoch 15: 0.0727

Epoch 16
	Step 0: mean loss = 0.0738
	Step 50: mean loss = 0.0723
	Step 100: mean loss = 0.0725
	Step 150: mean loss = 0.0725

	Loss on validation set after epoch 16: 0.0738

Epoch 17
	Step 0: mean loss = 0.0712
	Step 50: mean loss = 0.0721
	Step 100: mean loss = 0.0723
	Step 150: mean loss = 0.0723

	Loss on validation set after epoch 17: 0.0719

Epoch 18
	Step 0: mean loss = 0.0669
	Step 50: mean loss = 0.0721
	Step 100: mean loss = 0.0723
	Step 150: mean loss = 0.0722

	Loss on validation set after epoch 18: 0.0734

Epoch 19
	Step 0: mean loss = 0.0713
	Step 50: mean loss = 0.0719
	Step 100: mean loss = 0.0720
	Step 150: mean loss = 0.0720

	Loss on validation set after epoch 19: 0.0720

Epoch 20
	Step 0: mean loss = 0.0723
	Step 50: mean loss = 0.0717
	Step 100: mean loss = 0.0720
	Step 150: mean loss = 0.0719

	Loss on validation set after epoch 20: 0.0727

Epoch 21
	Step 0: mean loss = 0.0707
	Step 50: mean loss = 0.0715
	Step 100: mean loss = 0.0718
	Step 150: mean loss = 0.0718

	Loss on validation set after epoch 21: 0.0709

Epoch 22
	Step 0: mean loss = 0.0686
	Step 50: mean loss = 0.0715
	Step 100: mean loss = 0.0715
	Step 150: mean loss = 0.0716

	Loss on validation set after epoch 22: 0.0720

Epoch 23
	Step 0: mean loss = 0.0738
	Step 50: mean loss = 0.0713
	Step 100: mean loss = 0.0714
	Step 150: mean loss = 0.0714

	Loss on validation set after epoch 23: 0.0706

Epoch 24
	Step 0: mean loss = 0.0729
	Step 50: mean loss = 0.0713
	Step 100: mean loss = 0.0714
	Step 150: mean loss = 0.0714

	Loss on validation set after epoch 24: 0.0714

Epoch 25
	Step 0: mean loss = 0.0718
	Step 50: mean loss = 0.0708
	Step 100: mean loss = 0.0713
	Step 150: mean loss = 0.0712

	Loss on validation set after epoch 25: 0.0710

Epoch 26
	Step 0: mean loss = 0.0672
	Step 50: mean loss = 0.0708
	Step 100: mean loss = 0.0709
	Step 150: mean loss = 0.0711

	Loss on validation set after epoch 26: 0.0710

Epoch 27
	Step 0: mean loss = 0.0720
	Step 50: mean loss = 0.0707
	Step 100: mean loss = 0.0708
	Step 150: mean loss = 0.0709

	Loss on validation set after epoch 27: 0.0710

Epoch 28
	Step 0: mean loss = 0.0758
	Step 50: mean loss = 0.0704
	Step 100: mean loss = 0.0707
	Step 150: mean loss = 0.0708

	Loss on validation set after epoch 28: 0.0716

Epoch 29
	Step 0: mean loss = 0.0718
	Step 50: mean loss = 0.0705
	Step 100: mean loss = 0.0707
	Step 150: mean loss = 0.0706

	Loss on validation set after epoch 29: 0.0708

Epoch 30
	Step 0: mean loss = 0.0639
	Step 50: mean loss = 0.0702
	Step 100: mean loss = 0.0704
	Step 150: mean loss = 0.0705

	Loss on validation set after epoch 30: 0.0711

Epoch 31
	Step 0: mean loss = 0.0667
	Step 50: mean loss = 0.0702
	Step 100: mean loss = 0.0704
	Step 150: mean loss = 0.0704

	Loss on validation set after epoch 31: 0.0710

Epoch 32
	Step 0: mean loss = 0.0658
	Step 50: mean loss = 0.0703
	Step 100: mean loss = 0.0702
	Step 150: mean loss = 0.0703

	Loss on validation set after epoch 32: 0.0702

Epoch 33
	Step 0: mean loss = 0.0730
	Step 50: mean loss = 0.0700
	Step 100: mean loss = 0.0702
	Step 150: mean loss = 0.0703

	Loss on validation set after epoch 33: 0.0704

Epoch 34
	Step 0: mean loss = 0.0704
	Step 50: mean loss = 0.0700
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0702

	Loss on validation set after epoch 34: 0.0703

Epoch 35
	Step 0: mean loss = 0.0741
	Step 50: mean loss = 0.0701
	Step 100: mean loss = 0.0702
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 35: 0.0715

Epoch 36
	Step 0: mean loss = 0.0729
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 36: 0.0707

Epoch 37
	Step 0: mean loss = 0.0724
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 37: 0.0704

Epoch 38
	Step 0: mean loss = 0.0691
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 38: 0.0701

Epoch 39
	Step 0: mean loss = 0.0679
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 39: 0.0700

Epoch 40
	Step 0: mean loss = 0.0727
	Step 50: mean loss = 0.0697
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 40: 0.0701

Epoch 41
	Step 0: mean loss = 0.0758
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 41: 0.0702

Epoch 42
	Step 0: mean loss = 0.0666
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 42: 0.0705

Epoch 43
	Step 0: mean loss = 0.0680
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 43: 0.0699

Epoch 44
	Step 0: mean loss = 0.0662
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 44: 0.0705

Epoch 45
	Step 0: mean loss = 0.0671
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 45: 0.0698

Epoch 46
	Step 0: mean loss = 0.0691
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 46: 0.0703

Epoch 47
	Step 0: mean loss = 0.0688
	Step 50: mean loss = 0.0697
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 47: 0.0711

Epoch 48
	Step 0: mean loss = 0.0702
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 48: 0.0699

Epoch 49
	Step 0: mean loss = 0.0723
	Step 50: mean loss = 0.0700
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 49: 0.0702

Epoch 50
	Step 0: mean loss = 0.0722
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0699
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 50: 0.0708

Epoch 51
	Step 0: mean loss = 0.0690
	Step 50: mean loss = 0.0701
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 51: 0.0705

Epoch 52
	Step 0: mean loss = 0.0732
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 52: 0.0693

Epoch 53
	Step 0: mean loss = 0.0689
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0702
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 53: 0.0706

Epoch 54
	Step 0: mean loss = 0.0685
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 54: 0.0710

Epoch 55
	Step 0: mean loss = 0.0701
	Step 50: mean loss = 0.0697
	Step 100: mean loss = 0.0699
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 55: 0.0700

Epoch 56
	Step 0: mean loss = 0.0699
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 56: 0.0706

Epoch 57
	Step 0: mean loss = 0.0672
	Step 50: mean loss = 0.0700
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 57: 0.0704

Epoch 58
	Step 0: mean loss = 0.0684
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 58: 0.0705

Epoch 59
	Step 0: mean loss = 0.0695
	Step 50: mean loss = 0.0700
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 59: 0.0695

Epoch 60
	Step 0: mean loss = 0.0694
	Step 50: mean loss = 0.0700
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 60: 0.0697

Epoch 61
	Step 0: mean loss = 0.0710
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 61: 0.0704

Epoch 62
	Step 0: mean loss = 0.0722
	Step 50: mean loss = 0.0697
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 62: 0.0693

Epoch 63
	Step 0: mean loss = 0.0710
	Step 50: mean loss = 0.0697
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 63: 0.0703

Epoch 64
	Step 0: mean loss = 0.0655
	Step 50: mean loss = 0.0697
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 64: 0.0702

Epoch 65
	Step 0: mean loss = 0.0706
	Step 50: mean loss = 0.0700
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 65: 0.0698

Epoch 66
	Step 0: mean loss = 0.0684
	Step 50: mean loss = 0.0697
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 66: 0.0694

Epoch 67
	Step 0: mean loss = 0.0679
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 67: 0.0700

Epoch 68
	Step 0: mean loss = 0.0683
	Step 50: mean loss = 0.0701
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 68: 0.0693

Epoch 69
	Step 0: mean loss = 0.0724
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 69: 0.0701

Epoch 70
	Step 0: mean loss = 0.0690
	Step 50: mean loss = 0.0700
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 70: 0.0702

Epoch 71
	Step 0: mean loss = 0.0694
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 71: 0.0697

Epoch 72
	Step 0: mean loss = 0.0702
	Step 50: mean loss = 0.0697
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 72: 0.0698

Epoch 73
	Step 0: mean loss = 0.0684
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 73: 0.0702

Epoch 74
	Step 0: mean loss = 0.0651
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 74: 0.0709

Epoch 75
	Step 0: mean loss = 0.0665
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0701
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 75: 0.0699

Epoch 76
	Step 0: mean loss = 0.0736
	Step 50: mean loss = 0.0699
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0701

	Loss on validation set after epoch 76: 0.0706

Epoch 77
	Step 0: mean loss = 0.0645
	Step 50: mean loss = 0.0696
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 77: 0.0693

Epoch 78
	Step 0: mean loss = 0.0690
	Step 50: mean loss = 0.0698
	Step 100: mean loss = 0.0700
	Step 150: mean loss = 0.0700

	Loss on validation set after epoch 78: 0.0701

Model from epoch 68 was selected by early stopping.
Training process will be stopped now.
Deleted temporary files in ../data/trained_models/temp_01-03-16-42-31/
Location of saved model: ../data/trained_models/last_hope_71_01-03-16-42-31/ 

Adding feature wise convolutions with 32 filters per feature, 3 kernels and 1 strides ...
Adding feature wise convolutions with 1 filters per feature, 3 kernels and 1 strides ...
Adding feature based graph attention layer ...
Adding time based graph attention layer ...
Adding GRU layer with 256 units ...
Adding GRU layer with 61 units ...
Adding VAE as reconstruction model dimensions 3072 & 1536 and ...
Adding GRU layer to forecasting model with 512 units ...
Adding GRU layer to forecasting model with 256 units ...
Adding GRU layer to forecasting model with 61 units ...
VAE reconstructs the output of the gru layer.

Grid search running in anomaly detection model selection mode.
The following hyperparameters will not be evaluated:
relevance_mapping
unaffected_component_threshold
si_mode
si_parameter

Testing 294 combinations via grid search 
for model last_hope_71_01-03-16-42-31/ on the validation dataset. 


Top 50 combinations tested:
      gamma  single_timestamp_a.. affected_timestamp..     AD F1     AD F2    AD TPR   AD Prec    AD ACC    AD TNR    AD FPR  SI/ST AVG-HR@100%  SI/ST AVG-HR@150%  SI/ST AVG-HR@K  SI/ST F1  SI/ST TPR  SI/ST ACC
Comb                                                                                                                                                                                                              
162     0.6                 0.250                  110  0.479581  0.593264  0.704615  0.363492  0.709357  0.710469  0.289531           0.474993           0.549342        0.575385  0.218462   0.218462   0.879763
272     0.8                 0.275                  160  0.477733  0.601121  0.726154  0.355958  0.698246  0.691697  0.308303           0.480773           0.557561        0.584615  0.227692   0.227692   0.881183
161     0.6                 0.250                  100  0.476386  0.595177  0.713846  0.357473  0.701754  0.698917  0.301083           0.474993           0.549342        0.575385  0.218462   0.218462   0.879763
271     0.8                 0.275                  150  0.472112  0.598787  0.729231  0.349043  0.690058  0.680866  0.319134           0.480773           0.557561        0.584615  0.227692   0.227692   0.881183
163     0.6                 0.250                  120  0.470588  0.575916  0.676923  0.360656  0.710526  0.718412  0.281588           0.474993           0.549342        0.575385  0.218462   0.218462   0.879763
270     0.8                 0.275                  140  0.468293  0.600000  0.738462  0.342857  0.681287  0.667870  0.332130           0.480773           0.557561        0.584615  0.227692   0.227692   0.881183
269     0.8                 0.275                  130  0.463158  0.599010  0.744615  0.336111  0.671930  0.654874  0.345126           0.480773           0.557561        0.584615  0.227692   0.227692   0.881183
268     0.8                 0.275                  120  0.455910  0.595296  0.747692  0.327935  0.660819  0.640433  0.359567           0.480773           0.557561        0.584615  0.227692   0.227692   0.881183
164     0.6                 0.250                  130  0.454148  0.549974  0.640000  0.351946  0.707602  0.723466  0.276534           0.474993           0.549342        0.575385  0.218462   0.218462   0.879763
267     0.8                 0.275                  110  0.453382  0.599323  0.763077  0.322497  0.650292  0.623827  0.376173           0.480773           0.557561        0.584615  0.227692   0.227692   0.881183
165     0.6                 0.250                  140  0.451542  0.544344  0.630769  0.351630  0.708772  0.727076  0.272924           0.474993           0.549342        0.575385  0.218462   0.218462   0.879763
110     0.5                 0.225                  150  0.448367  0.602467  0.781538  0.314356  0.634503  0.600000  0.400000           0.482331           0.562650        0.587692  0.230769   0.230769   0.881657
266     0.8                 0.275                  100  0.447628  0.597514  0.769231  0.315657  0.639181  0.608664  0.391336           0.480773           0.557561        0.584615  0.227692   0.227692   0.881183
166     0.6                 0.250                  150  0.446449  0.531686  0.609231  0.352313  0.712865  0.737184  0.262816           0.474993           0.549342        0.575385  0.218462   0.218462   0.879763
111     0.5                 0.225                  160  0.445837  0.595124  0.766154  0.314394  0.638012  0.607942  0.392058           0.482331           0.562650        0.587692  0.230769   0.230769   0.881657
109     0.5                 0.225                  140  0.442142  0.600094  0.787692  0.307323  0.622222  0.583394  0.416606           0.482331           0.562650        0.587692  0.230769   0.230769   0.881657
167     0.6                 0.250                  160  0.439024  0.514706  0.581538  0.352612  0.717544  0.749458  0.250542           0.474993           0.549342        0.575385  0.218462   0.218462   0.879763
108     0.5                 0.225                  130  0.434489  0.595459  0.790769  0.299534  0.608772  0.566065  0.433935           0.482331           0.562650        0.587692  0.230769   0.230769   0.881657
107     0.5                 0.225                  120  0.428691  0.591076  0.790769  0.294050  0.599415  0.554513  0.445487           0.482331           0.562650        0.587692  0.230769   0.230769   0.881657
160     0.6                 0.225                  160  0.427623  0.648932  0.990769  0.272650  0.495906  0.379783  0.620217           0.486426           0.574621        0.587692  0.240000   0.240000   0.883077
159     0.6                 0.225                  150  0.425644  0.647106  0.990769  0.271044  0.491813  0.374729  0.625271           0.486426           0.574621        0.587692  0.240000   0.240000   0.883077
158     0.6                 0.225                  140  0.423963  0.645549  0.990769  0.269682  0.488304  0.370397  0.629603           0.486426           0.574621        0.587692  0.240000   0.240000   0.883077
157     0.6                 0.225                  130  0.423684  0.645291  0.990769  0.269456  0.487719  0.369675  0.630325           0.486426           0.574621        0.587692  0.240000   0.240000   0.883077
210     0.7                 0.250                  100  0.423266  0.628643  0.929231  0.274047  0.518713  0.422383  0.577617           0.487437           0.570794        0.587692  0.236923   0.236923   0.882604
156     0.6                 0.225                  120  0.423052  0.645484  0.993846  0.268719  0.484795  0.365343  0.634657           0.486426           0.574621        0.587692  0.240000   0.240000   0.883077
106     0.5                 0.225                  110  0.423045  0.586758  0.790769  0.288764  0.590058  0.542960  0.457040           0.482331           0.562650        0.587692  0.230769   0.230769   0.881657
105     0.5                 0.225                  100  0.421481  0.587568  0.796923  0.286504  0.584211  0.534296  0.465704           0.482331           0.562650        0.587692  0.230769   0.230769   0.881657
155     0.6                 0.225                  110  0.421396  0.643939  0.993846  0.267384  0.481287  0.361011  0.638989           0.486426           0.574621        0.587692  0.240000   0.240000   0.883077
55      0.4                 0.200                  160  0.420402  0.599383  0.836923  0.280702  0.561404  0.496751  0.503249           0.486312           0.569471        0.578462  0.230769   0.230769   0.881657
51      0.4                 0.200                  120  0.419476  0.606061  0.861538  0.277228  0.546784  0.472924  0.527076           0.486312           0.569471        0.578462  0.230769   0.230769   0.881657
50      0.4                 0.200                  110  0.419331  0.607759  0.867692  0.276471  0.543275  0.467148  0.532852           0.486312           0.569471        0.578462  0.230769   0.230769   0.881657
52      0.4                 0.200                  130  0.419306  0.604085  0.855385  0.277722  0.549708  0.477978  0.522022           0.486312           0.569471        0.578462  0.230769   0.230769   0.881657
215     0.7                 0.250                  150  0.419006  0.615482  0.895385  0.273496  0.528070  0.441877  0.558123           0.487437           0.570794        0.587692  0.236923   0.236923   0.882604
211     0.7                 0.250                  110  0.418967  0.619765  0.910769  0.272059  0.519883  0.428159  0.571841           0.487437           0.570794        0.587692  0.236923   0.236923   0.882604
54      0.4                 0.200                  150  0.418888  0.600962  0.846154  0.278340  0.553801  0.485199  0.514801           0.486312           0.569471        0.578462  0.230769   0.230769   0.881657
49      0.4                 0.200                  100  0.418879  0.609181  0.873846  0.275461  0.539181  0.460650  0.539350           0.486312           0.569471        0.578462  0.230769   0.230769   0.881657
216     0.7                 0.250                  160  0.418841  0.613588  0.889231  0.273934  0.530994  0.446931  0.553069           0.487437           0.570794        0.587692  0.236923   0.236923   0.882604
214     0.7                 0.250                  140  0.418338  0.615774  0.898462  0.272642  0.525146  0.437545  0.562455           0.487437           0.570794        0.587692  0.236923   0.236923   0.882604
53      0.4                 0.200                  140  0.418182  0.601307  0.849231  0.277387  0.550877  0.480866  0.519134           0.486312           0.569471        0.578462  0.230769   0.230769   0.881657
212     0.7                 0.250                  120  0.417910  0.617128  0.904615  0.271719  0.521053  0.431047  0.568953           0.487437           0.570794        0.587692  0.236923   0.236923   0.882604
154     0.6                 0.225                  100  0.417853  0.640619  0.993846  0.264537  0.473684  0.351625  0.648375           0.486426           0.574621        0.587692  0.240000   0.240000   0.883077
213     0.7                 0.250                  130  0.417143  0.614737  0.898462  0.271628  0.522807  0.434657  0.565343           0.487437           0.570794        0.587692  0.236923   0.236923   0.882604
104     0.5                 0.200                  160  0.401235  0.626204  1.000000  0.250965  0.432749  0.299639  0.700361           0.487782           0.575566        0.584615  0.240000   0.240000   0.883077
103     0.5                 0.200                  150  0.399754  0.624760  1.000000  0.249808  0.429240  0.295307  0.704693           0.487782           0.575566        0.584615  0.240000   0.240000   0.883077
102     0.5                 0.200                  140  0.396100  0.621177  1.000000  0.246960  0.420468  0.284477  0.715523           0.487782           0.575566        0.584615  0.240000   0.240000   0.883077
101     0.5                 0.200                  130  0.395137  0.620229  1.000000  0.246212  0.418129  0.281588  0.718412           0.487782           0.575566        0.584615  0.240000   0.240000   0.883077
100     0.5                 0.200                  120  0.390625  0.615764  1.000000  0.242718  0.407018  0.267870  0.732130           0.487782           0.575566        0.584615  0.240000   0.240000   0.883077
265     0.8                 0.250                  160  0.388524  0.613671  1.000000  0.241098  0.401754  0.261372  0.738628           0.487665           0.572350        0.581538  0.240000   0.240000   0.883077
99      0.5                 0.200                  110  0.386445  0.611592  1.000000  0.239499  0.396491  0.254874  0.745126           0.487782           0.575566        0.584615  0.240000   0.240000   0.883077
264     0.8                 0.250                  150  0.382578  0.607704  1.000000  0.236536  0.386550  0.242599  0.757401           0.487665           0.572350        0.581538  0.240000   0.240000   0.883077

Full result output for the best combination:
                 #Examples   TP   FP   TN  FN       ACC       FNR       TNR       FPR       TPR      Prec        F1        F2  AVG # affected
Component                                                                                                                                    
no_failure            1385    0  401  984   0  0.710469       NaN  0.710469  0.289531       NaN  0.000000       NaN       NaN       99.353069
txt15_i1                 5    5    0    0   0  1.000000  0.000000       NaN       NaN  1.000000  1.000000  1.000000  1.000000      411.200000
txt15_i3                 5    1    0    0   4  0.200000  0.800000       NaN       NaN  0.200000  1.000000  0.333333  0.238095       28.400000
txt15_conveyor           3    3    0    0   0  1.000000  0.000000       NaN       NaN  1.000000  1.000000  1.000000  1.000000      207.000000
txt15_m1               160  153    0    0   7  0.956250  0.043750       NaN       NaN  0.956250  1.000000  0.977636  0.964691      319.000000
txt15_pl                 9    2    0    0   7  0.222222  0.777778       NaN       NaN  0.222222  1.000000  0.363636  0.263158       45.666667
txt16_i3                 4    0    0    0   4  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        3.750000
txt16_conveyor           8    0    0    0   8  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN       13.125000
txt16_m3                65   49    0    0  16  0.753846  0.246154       NaN       NaN  0.753846  1.000000  0.859649  0.792880      198.076923
txt16_turntable          2    0    0    0   2  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        4.500000
txt17_i1                16    9    0    0   7  0.562500  0.437500       NaN       NaN  0.562500  1.000000  0.720000  0.616438      260.750000
txt17_pl                14    0    0    0  14  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN       10.357143
txt18_pl                28    7    0    0  21  0.250000  0.750000       NaN       NaN  0.250000  1.000000  0.400000  0.294118       50.535714
txt19_i4                 6    0    0    0   6  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        4.500000
combined              1710  229  401  984  96  0.709357  0.295385  0.710469  0.289531  0.704615  0.363492  0.479581  0.593264      123.179532

                 #Examples   TP   FP   TN  FN       ACC       FNR       TNR       FPR       TPR      Prec        F1        F2  AVG # affected
Component                                                                                                                                    
no_failure            1385    0  401  984   0  0.710469       NaN  0.710469  0.289531       NaN  0.000000       NaN       NaN       99.353069
txt15_i1                 5    5    0    0   0  1.000000  0.000000       NaN       NaN  1.000000  1.000000  1.000000  1.000000      411.200000
txt15_i3                 5    1    0    0   4  0.200000  0.800000       NaN       NaN  0.200000  1.000000  0.333333  0.238095       28.400000
txt15_conveyor           3    3    0    0   0  1.000000  0.000000       NaN       NaN  1.000000  1.000000  1.000000  1.000000      207.000000
txt15_m1               160  153    0    0   7  0.956250  0.043750       NaN       NaN  0.956250  1.000000  0.977636  0.964691      319.000000
txt15_pl                 9    2    0    0   7  0.222222  0.777778       NaN       NaN  0.222222  1.000000  0.363636  0.263158       45.666667
txt16_i3                 4    0    0    0   4  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        3.750000
txt16_conveyor           8    0    0    0   8  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN       13.125000
txt16_m3                65   49    0    0  16  0.753846  0.246154       NaN       NaN  0.753846  1.000000  0.859649  0.792880      198.076923
txt16_turntable          2    0    0    0   2  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        4.500000
txt17_i1                16    9    0    0   7  0.562500  0.437500       NaN       NaN  0.562500  1.000000  0.720000  0.616438      260.750000
txt17_pl                14    0    0    0  14  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN       10.357143
txt18_pl                28    7    0    0  21  0.250000  0.750000       NaN       NaN  0.250000  1.000000  0.400000  0.294118       50.535714
txt19_i4                 6    0    0    0   6  0.000000  1.000000       NaN       NaN  0.000000       NaN       NaN       NaN        4.500000
combined              1710  229  401  984  96  0.709357  0.295385  0.710469  0.289531  0.704615  0.363492  0.479581  0.593264      123.179532

Execution time: 6510.484980884008
