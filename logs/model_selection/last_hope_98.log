
Dataset loaded:
Shape of training set (example, time, channels): (20812, 500, 61)
Shape of test set (example, time, channels): (3989, 500, 61)
Shape of train validation set (example, time, channels): (2313, 500, 61)
Shape of test validation set (example, time, channels): (1710, 500, 61)
Num of classes in all: 25

Creating model based on ../configuration/hyperparameter_combinations/last_hope_97.json hyperparameter file 

Adding feature wise convolutions with 32 filters per feature, 3 kernels and 1 strides ...
Adding feature wise convolutions with 1 filters per feature, 3 kernels and 1 strides ...
Adding feature based graph attention layer ...
Adding time based graph attention layer ...
Adding GRU layer with 256 units ...
Adding GRU layer with 61 units ...
Adding GRU AE as reconstruction model ...
Adding dense layer to forecasting model with 512 units and ReLu activation ...
Adding dense layer to forecasting model with 256 units and ReLu activation ...
Adding last dense layer to forecasting model with 61 units and sigmoid activation ...
VAE reconstructs the output of the gru layer.

Epoch 0
	Step 0: mean loss = 0.2199
	Step 50: mean loss = 0.2031
	Step 100: mean loss = 0.1444
	Step 150: mean loss = 0.1201

	Loss on validation set after epoch 0: 0.0697

Epoch 1
	Step 0: mean loss = 0.0674
	Step 50: mean loss = 0.0691
	Step 100: mean loss = 0.0689
	Step 150: mean loss = 0.0685

	Loss on validation set after epoch 1: 0.0652

Epoch 2
	Step 0: mean loss = 0.0643
	Step 50: mean loss = 0.0636
	Step 100: mean loss = 0.0628
	Step 150: mean loss = 0.0619

	Loss on validation set after epoch 2: 0.0592

Epoch 3
	Step 0: mean loss = 0.0606
	Step 50: mean loss = 0.0579
	Step 100: mean loss = 0.0570
	Step 150: mean loss = 0.0563

	Loss on validation set after epoch 3: 0.0540

Epoch 4
	Step 0: mean loss = 0.0543
	Step 50: mean loss = 0.0536
	Step 100: mean loss = 0.0537
	Step 150: mean loss = 0.0535

	Loss on validation set after epoch 4: 0.0520

Epoch 5
	Step 0: mean loss = 0.0542
	Step 50: mean loss = 0.0525
	Step 100: mean loss = 0.0524
	Step 150: mean loss = 0.0523

	Loss on validation set after epoch 5: 0.0521

Epoch 6
	Step 0: mean loss = 0.0554
	Step 50: mean loss = 0.0514
	Step 100: mean loss = 0.0513
	Step 150: mean loss = 0.0512

	Loss on validation set after epoch 6: 0.0500

Epoch 7
	Step 0: mean loss = 0.0497
	Step 50: mean loss = 0.0505
	Step 100: mean loss = 0.0504
	Step 150: mean loss = 0.0502

	Loss on validation set after epoch 7: 0.0489

Epoch 8
	Step 0: mean loss = 0.0438
	Step 50: mean loss = 0.0492
	Step 100: mean loss = 0.0491
	Step 150: mean loss = 0.0488

	Loss on validation set after epoch 8: 0.0474

Epoch 9
	Step 0: mean loss = 0.0441
	Step 50: mean loss = 0.0475
	Step 100: mean loss = 0.0630
	Step 150: mean loss = 0.0671

	Loss on validation set after epoch 9: 0.0747

Epoch 10
	Step 0: mean loss = 0.0730
	Step 50: mean loss = 0.0744
	Step 100: mean loss = 0.0742
	Step 150: mean loss = 0.0739

	Loss on validation set after epoch 10: 0.0723

Epoch 11
	Step 0: mean loss = 0.0739
	Step 50: mean loss = 0.0719
	Step 100: mean loss = 0.0707
	Step 150: mean loss = 0.0683

	Loss on validation set after epoch 11: 0.0622

Epoch 12
	Step 0: mean loss = 0.0552
	Step 50: mean loss = 0.0702
	Step 100: mean loss = 0.0647
	Step 150: mean loss = 0.0623

	Loss on validation set after epoch 12: 0.0545

Epoch 13
	Step 0: mean loss = 0.0543
	Step 50: mean loss = 0.0532
	Step 100: mean loss = 0.0527
	Step 150: mean loss = 0.0524

	Loss on validation set after epoch 13: 0.0511

Epoch 14
	Step 0: mean loss = 0.0492
	Step 50: mean loss = 0.0500
	Step 100: mean loss = 0.0500
	Step 150: mean loss = 0.0499

	Loss on validation set after epoch 14: 0.0489

Epoch 15
	Step 0: mean loss = 0.0543
	Step 50: mean loss = 0.0482
	Step 100: mean loss = 0.0482
	Step 150: mean loss = 0.0479

	Loss on validation set after epoch 15: 0.0463

Epoch 16
	Step 0: mean loss = 0.0432
	Step 50: mean loss = 0.0470
	Step 100: mean loss = 0.0469
	Step 150: mean loss = 0.0467

	Loss on validation set after epoch 16: 0.0461

Epoch 17
	Step 0: mean loss = 0.0435
	Step 50: mean loss = 0.0461
	Step 100: mean loss = 0.0463
	Step 150: mean loss = 0.0469

	Loss on validation set after epoch 17: 0.0453

Epoch 18
	Step 0: mean loss = 0.0447
	Step 50: mean loss = 0.0452
	Step 100: mean loss = 0.0449
	Step 150: mean loss = 0.0448

	Loss on validation set after epoch 18: 0.0439

Epoch 19
	Step 0: mean loss = 0.0465
	Step 50: mean loss = 0.0433
	Step 100: mean loss = 0.0436
	Step 150: mean loss = 0.0434

	Loss on validation set after epoch 19: 0.0437

Epoch 20
	Step 0: mean loss = 0.0414
	Step 50: mean loss = 0.0424
	Step 100: mean loss = 0.0424
	Step 150: mean loss = 0.0423

	Loss on validation set after epoch 20: 0.0417

Epoch 21
	Step 0: mean loss = 0.0431
	Step 50: mean loss = 0.0412
	Step 100: mean loss = 0.0410
	Step 150: mean loss = 0.0410

	Loss on validation set after epoch 21: 0.0400

Epoch 22
	Step 0: mean loss = 0.0385
	Step 50: mean loss = 0.0399
	Step 100: mean loss = 0.0397
	Step 150: mean loss = 0.0398

	Loss on validation set after epoch 22: 0.0399

Epoch 23
	Step 0: mean loss = 0.0380
	Step 50: mean loss = 0.0392
	Step 100: mean loss = 0.0393
	Step 150: mean loss = 0.0392

	Loss on validation set after epoch 23: 0.0383

Epoch 24
	Step 0: mean loss = 0.0373
	Step 50: mean loss = 0.0387
	Step 100: mean loss = 0.0387
	Step 150: mean loss = 0.0387

	Loss on validation set after epoch 24: 0.0382

Epoch 25
	Step 0: mean loss = 0.0399
	Step 50: mean loss = 0.0382
	Step 100: mean loss = 0.0380
	Step 150: mean loss = 0.0379

	Loss on validation set after epoch 25: 0.0376

Epoch 26
	Step 0: mean loss = 0.0410
	Step 50: mean loss = 0.0370
	Step 100: mean loss = 0.0371
	Step 150: mean loss = 0.0371

	Loss on validation set after epoch 26: 0.0380

Epoch 27
	Step 0: mean loss = 0.0375
	Step 50: mean loss = 0.0362
	Step 100: mean loss = 0.0364
	Step 150: mean loss = 0.0364

	Loss on validation set after epoch 27: 0.0365

Epoch 28
	Step 0: mean loss = 0.0398
	Step 50: mean loss = 0.0360
	Step 100: mean loss = 0.0359
	Step 150: mean loss = 0.0361

	Loss on validation set after epoch 28: 0.0358

Epoch 29
	Step 0: mean loss = 0.0333
	Step 50: mean loss = 0.0355
	Step 100: mean loss = 0.0355
	Step 150: mean loss = 0.0356

	Loss on validation set after epoch 29: 0.0350

Epoch 30
	Step 0: mean loss = 0.0326
	Step 50: mean loss = 0.0352
	Step 100: mean loss = 0.0353
	Step 150: mean loss = 0.0356

	Loss on validation set after epoch 30: 0.0377

Epoch 31
	Step 0: mean loss = 0.0321
	Step 50: mean loss = 0.0361
	Step 100: mean loss = 0.0362
	Step 150: mean loss = 0.0362

	Loss on validation set after epoch 31: 0.0384

Epoch 32
	Step 0: mean loss = 0.0377
	Step 50: mean loss = 0.0370
	Step 100: mean loss = 0.0399
	Step 150: mean loss = 0.0410

	Loss on validation set after epoch 32: 0.0421

Epoch 33
	Step 0: mean loss = 0.0379
	Step 50: mean loss = 0.0420
	Step 100: mean loss = 0.0416
	Step 150: mean loss = 0.0420

	Loss on validation set after epoch 33: 0.0414

Epoch 34
	Step 0: mean loss = 0.0407
	Step 50: mean loss = 0.0406
	Step 100: mean loss = 0.0402
	Step 150: mean loss = 0.0411

	Loss on validation set after epoch 34: 0.0440

Epoch 35
	Step 0: mean loss = 0.0498
	Step 50: mean loss = 0.0426
	Step 100: mean loss = 0.0426
	Step 150: mean loss = 0.0428

	Loss on validation set after epoch 35: 0.0431

Epoch 36
	Step 0: mean loss = 0.0428
	Step 50: mean loss = 0.0421
	Step 100: mean loss = 0.0423
	Step 150: mean loss = 0.0426

	Loss on validation set after epoch 36: 0.0423

Epoch 37
	Step 0: mean loss = 0.0423
	Step 50: mean loss = 0.0424
	Step 100: mean loss = 0.0425
	Step 150: mean loss = 0.0425

	Loss on validation set after epoch 37: 0.0417

Epoch 38
	Step 0: mean loss = 0.0446
	Step 50: mean loss = 0.0417
	Step 100: mean loss = 0.0422
	Step 150: mean loss = 0.0421

	Loss on validation set after epoch 38: 0.0422

Epoch 39
	Step 0: mean loss = 0.0429
	Step 50: mean loss = 0.0414
	Step 100: mean loss = 0.0414
	Step 150: mean loss = 0.0419

	Loss on validation set after epoch 39: 0.0425

Model from epoch 29 was selected by early stopping.
Training process will be stopped now.
Deleted temporary files in ../data/trained_models/temp_01-05-16-04-08/
Location of saved model: ../data/trained_models/last_hope_97_01-05-16-04-08/ 

Adding feature wise convolutions with 32 filters per feature, 3 kernels and 1 strides ...
Adding feature wise convolutions with 1 filters per feature, 3 kernels and 1 strides ...
Adding feature based graph attention layer ...
Adding time based graph attention layer ...
Adding GRU layer with 256 units ...
Adding GRU layer with 61 units ...
Adding GRU AE as reconstruction model ...
Adding dense layer to forecasting model with 512 units and ReLu activation ...
Adding dense layer to forecasting model with 256 units and ReLu activation ...
Adding last dense layer to forecasting model with 61 units and sigmoid activation ...
VAE reconstructs the output of the gru layer.

Grid search running in anomaly detection model selection mode.
The following hyperparameters will not be evaluated:
relevance_mapping
unaffected_component_threshold
si_mode
si_parameter

Testing 294 combinations via grid search 
for model last_hope_97_01-05-16-04-08/ on the validation dataset. 


Top 50 combinations tested:
      gamma  single_timestamp_a.. affected_timestamp..     AD F1     AD F2    AD TPR  AD Prec    AD ACC  AD TNR  AD FPR  SI/ST AVG-HR@100%  SI/ST AVG-HR@150%  SI/ST AVG-HR@K  SI/ST F1  SI/ST TPR  SI/ST ACC
Comb                                                                                                                                                                                                         
0       0.3                 0.200                  100  0.006135  0.003843  0.003077      1.0  0.810526     1.0     0.0           0.161708           0.245748        0.163077  0.027692   0.027692   0.850414
1       0.3                 0.200                  110  0.006135  0.003843  0.003077      1.0  0.810526     1.0     0.0           0.161708           0.245748        0.163077  0.027692   0.027692   0.850414
2       0.3                 0.200                  120  0.006135  0.003843  0.003077      1.0  0.810526     1.0     0.0           0.161708           0.245748        0.163077  0.027692   0.027692   0.850414
3       0.3                 0.200                  130  0.006135  0.003843  0.003077      1.0  0.810526     1.0     0.0           0.161708           0.245748        0.163077  0.027692   0.027692   0.850414
4       0.3                 0.200                  140  0.006135  0.003843  0.003077      1.0  0.810526     1.0     0.0           0.161708           0.245748        0.163077  0.027692   0.027692   0.850414
5       0.3                 0.200                  150  0.006135  0.003843  0.003077      1.0  0.810526     1.0     0.0           0.161708           0.245748        0.163077  0.027692   0.027692   0.850414
6       0.3                 0.200                  160  0.006135  0.003843  0.003077      1.0  0.810526     1.0     0.0           0.161708           0.245748        0.163077  0.027692   0.027692   0.850414
49      0.4                 0.200                  100  0.006135  0.003843  0.003077      1.0  0.810526     1.0     0.0           0.157230           0.240825        0.150769  0.018462   0.018462   0.848994
50      0.4                 0.200                  110  0.006135  0.003843  0.003077      1.0  0.810526     1.0     0.0           0.157230           0.240825        0.150769  0.018462   0.018462   0.848994
7       0.3                 0.225                  100       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
8       0.3                 0.225                  110       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
9       0.3                 0.225                  120       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
10      0.3                 0.225                  130       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
11      0.3                 0.225                  140       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
12      0.3                 0.225                  150       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
13      0.3                 0.225                  160       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
14      0.3                 0.250                  100       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
15      0.3                 0.250                  110       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
16      0.3                 0.250                  120       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
17      0.3                 0.250                  130       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
18      0.3                 0.250                  140       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
19      0.3                 0.250                  150       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
20      0.3                 0.250                  160       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
21      0.3                 0.275                  100       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
22      0.3                 0.275                  110       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
23      0.3                 0.275                  120       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
24      0.3                 0.275                  130       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
25      0.3                 0.275                  140       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
26      0.3                 0.275                  150       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
27      0.3                 0.275                  160       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
28      0.3                 0.300                  100       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
29      0.3                 0.300                  110       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
30      0.3                 0.300                  120       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
31      0.3                 0.300                  130       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
32      0.3                 0.300                  140       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
33      0.3                 0.300                  150       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
34      0.3                 0.300                  160       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
35      0.3                 0.325                  100       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
36      0.3                 0.325                  110       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
37      0.3                 0.325                  120       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
38      0.3                 0.325                  130       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
39      0.3                 0.325                  140       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
40      0.3                 0.325                  150       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
41      0.3                 0.325                  160       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
42      0.3                 0.350                  100       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
43      0.3                 0.350                  110       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
44      0.3                 0.350                  120       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
45      0.3                 0.350                  130       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
46      0.3                 0.350                  140       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994
47      0.3                 0.350                  150       NaN       NaN  0.000000      NaN  0.809942     1.0     0.0           0.156307           0.239902        0.147692  0.018462   0.018462   0.848994

Full result output for the best combination:
                 #Examples  TP  FP    TN   FN       ACC       FNR  TNR  FPR       TPR  Prec        F1        F2  AVG # affected
Component                                                                                                                      
no_failure            1385   0   0  1385    0  1.000000       NaN  1.0  0.0       NaN   NaN       NaN       NaN        0.000000
txt15_i1                 5   1   0     0    4  0.200000  0.800000  NaN  NaN  0.200000   1.0  0.333333  0.238095       77.800000
txt15_i3                 5   0   0     0    5  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt15_conveyor           3   0   0     0    3  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt15_m1               160   0   0     0  160  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.412500
txt15_pl                 9   0   0     0    9  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt16_i3                 4   0   0     0    4  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt16_conveyor           8   0   0     0    8  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt16_m3                65   0   0     0   65  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt16_turntable          2   0   0     0    2  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt17_i1                16   0   0     0   16  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        4.750000
txt17_pl                14   0   0     0   14  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt18_pl                28   0   0     0   28  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt19_i4                 6   0   0     0    6  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
combined              1710   1   0  1385  324  0.810526  0.996923  1.0  0.0  0.003077   1.0  0.006135  0.003843        0.310526

                 #Examples  TP  FP    TN   FN       ACC       FNR  TNR  FPR       TPR  Prec        F1        F2  AVG # affected
Component                                                                                                                      
no_failure            1385   0   0  1385    0  1.000000       NaN  1.0  0.0       NaN   NaN       NaN       NaN        0.000000
txt15_i1                 5   1   0     0    4  0.200000  0.800000  NaN  NaN  0.200000   1.0  0.333333  0.238095       77.800000
txt15_i3                 5   0   0     0    5  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt15_conveyor           3   0   0     0    3  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt15_m1               160   0   0     0  160  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.412500
txt15_pl                 9   0   0     0    9  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt16_i3                 4   0   0     0    4  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt16_conveyor           8   0   0     0    8  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt16_m3                65   0   0     0   65  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt16_turntable          2   0   0     0    2  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt17_i1                16   0   0     0   16  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        4.750000
txt17_pl                14   0   0     0   14  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt18_pl                28   0   0     0   28  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
txt19_i4                 6   0   0     0    6  0.000000  1.000000  NaN  NaN  0.000000   NaN       NaN       NaN        0.000000
combined              1710   1   0  1385  324  0.810526  0.996923  1.0  0.0  0.003077   1.0  0.006135  0.003843        0.310526

Execution time: 3542.559543750016
